#+TITLE: Code Generation Evaluation Rubric
#+PROPERTY: Rubric-Type Code-Generation
#+PROPERTY: Version 1.0

* Overview

This rubric evaluates the quality of code generated by LLM agents across multiple dimensions. Each dimension is scored 0-3, with clear criteria for each score level.

* Scoring Dimensions

** 1. Correctness (Weight: 40%)

| Score | Criteria | Description |
|-------+----------+-------------|
| 0 | Non-functional | Code doesn't run or has syntax errors |
| 1 | Partially works | Basic functionality present but significant bugs |
| 2 | Mostly works | Works for main cases, edge cases have issues |
| 3 | Fully correct | Handles all cases including edge cases correctly |

*** Evaluation Methods
- Run test suite
- Manual testing of edge cases
- Input validation checking
- Error handling verification

** 2. Code Style (Weight: 20%)

| Score | Criteria | Description |
|-------+----------+-------------|
| 0 | Inconsistent | No consistent style, hard to read |
| 1 | Some consistency | Basic formatting present, some inconsistencies |
| 2 | Mostly consistent | Follows project style with minor deviations |
| 3 | Perfect style | Fully consistent with project conventions |

*** Evaluation Methods
- Linting tools
- Style guide compliance
- Naming conventions
- Code formatting

** 3. Performance (Weight: 15%)

| Score | Criteria | Description |
|-------+----------+-------------|
| 0 | Major issues | Obvious performance problems (O(nÂ³) when O(n) possible) |
| 1 | Inefficient | Works but has unnecessary operations |
| 2 | Good | Reasonable performance for the use case |
| 3 | Optimal | Best possible algorithmic approach |

*** Evaluation Methods
- Complexity analysis
- Benchmarking
- Resource usage
- Scalability testing

** 4. Maintainability (Weight: 15%)

| Score | Criteria | Description |
|-------+----------+-------------|
| 0 | Unmaintainable | Difficult to understand or modify |
| 1 | Poor | Some structure but hard to extend |
| 2 | Good | Clear structure, reasonable to modify |
| 3 | Excellent | Very clear, modular, easy to extend |

*** Evaluation Methods
- Code complexity metrics
- Documentation quality
- Separation of concerns
- DRY principle adherence

** 5. Test Coverage (Weight: 10%)

| Score | Criteria | Description |
|-------+----------+-------------|
| 0 | No tests | No tests provided |
| 1 | Minimal | Basic happy path tested |
| 2 | Good | Main cases and some edge cases tested |
| 3 | Comprehensive | Full coverage including edge cases |

*** Evaluation Methods
- Coverage tools
- Test quality review
- Edge case coverage
- Test maintainability

* Scoring Calculation

#+begin_src python
def calculate_score(scores, weights):
    """
    Calculate weighted average score.
    
    Args:
        scores: Dict of dimension -> score (0-3)
        weights: Dict of dimension -> weight (0-1)
    
    Returns:
        Final score (0-100)
    """
    total = sum(scores[dim] * weights[dim] for dim in scores)
    max_possible = sum(3 * weight for weight in weights.values())
    return (total / max_possible) * 100
#+end_src

* Usage Instructions

** 1. Prepare Test Environment
- Set up testbed
- Prepare test cases
- Configure evaluation tools

** 2. Generate Code
- Record generation session
- Save all outputs
- Note any errors/retries

** 3. Evaluate Each Dimension
- Use evaluation methods listed
- Assign scores objectively
- Document evidence for scores

** 4. Calculate Final Score
- Apply weights
- Compute weighted average
- Convert to percentage

** 5. Document Results
#+begin_src org
  * Evaluation Results
  :PROPERTIES:
  :Date: [2025-07-05]
  :Evaluator: [Name]
  :Session: [[file:session.txt][session.txt]]
  :END:
  
  ** Scores
  | Dimension | Score | Evidence |
  |-----------+-------+----------|
  | Correctness | 2 | Passes 18/20 tests |
  | Code Style | 3 | No linting errors |
  | Performance | 2 | O(n log n) solution |
  | Maintainability | 2 | Clear structure |
  | Test Coverage | 1 | Only happy path |
  
  ** Final Score: 72%
  
  ** Notes
  - Failed edge cases: empty input, unicode
  - Performance acceptable but could optimize memory usage
  - Tests need expansion
#+end_src

* Rubric Calibration

** Inter-rater Reliability
- Have multiple evaluators score same output
- Discuss discrepancies
- Refine criteria for clarity

** Baseline Establishment
- Score human-written code
- Score previous agent versions
- Track improvement over time

* Adaptation Guidelines

** For Specific Languages
Adjust style criteria for:
- Python: PEP 8 compliance
- JavaScript: ESLint rules
- Go: gofmt standards

** For Specific Domains
Add domain-specific criteria:
- Security: Add security dimension
- API Design: Add interface quality
- Frontend: Add UX considerations

** For Different Tasks
Weight adjustments:
- Bug fixes: Increase correctness weight
- Refactoring: Increase maintainability weight
- Performance: Increase performance weight

* Change Log

** Version 1.0 (2025-07-05)
- Initial rubric creation
- Five core dimensions
- Standard weights

* References

- [[https://www.acm.org/publications/policies/artifact-review-and-badging-current][ACM Artifact Evaluation]]
- [[https://github.com/google/eng-practices/blob/master/review/reviewer/standard.md][Google Code Review Standards]]
- Industry best practices