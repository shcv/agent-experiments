#+TITLE: Evaluation Directory

* Overview

This directory contains evaluation methods, rubrics, scripts, and metrics for assessing LLM agent performance. Our evaluation framework supports both quantitative measurements and qualitative analysis.

* Directory Structure

** methods/
Experimental evaluation methodologies:
- **experimental-evaluation-methods.org** - Four core evaluation approaches:
  - A/B Race: Parallel comparison of agents
  - Iterative Refinement: Specification improvement through retries
  - Hint-Based Interventions: Pattern-based assistance
  - Comprehensive Partial Credit: Detailed rubric scoring

** rubrics/
Standardized scoring criteria:
- **code-generation-rubric.org** - Evaluates generated code quality
- **task-completion-rubric.org** - Measures task fulfillment
- Additional rubrics can be added for specific domains

** scripts/
Automation tools for evaluation:
- Test runners
- Metric calculators
- Report generators
- Statistical analysis

** metrics/
Metric definitions and tracking:
- Performance metrics
- Quality metrics
- Behavioral metrics
- Comparative analytics

* Evaluation Philosophy

** Comprehensive Assessment
We evaluate multiple dimensions:
1. **Efficiency** - Speed and resource usage
2. **Correctness** - Accuracy and completeness
3. **Quality** - Code quality, maintainability
4. **Robustness** - Error handling, edge cases
5. **Understanding** - Task comprehension

** Reproducibility
All evaluations must be:
- Fully documented
- Independently reproducible
- Statistically valid
- Consistently applied

** Actionable Insights
Evaluations should produce:
- Clear performance metrics
- Identified failure patterns
- Improvement recommendations
- Comparative analysis

* Quick Start Guide

** 1. Choose Evaluation Method
Based on your research question:
- Model comparison? → Use A/B Race
- Prompt optimization? → Use Iterative Refinement
- Common failures? → Use Hint Interventions
- Capability assessment? → Use Partial Credit

** 2. Select or Create Rubric
- Use existing rubrics when applicable
- Adapt rubrics for specific needs
- Create new rubrics following templates

** 3. Run Evaluation
#+begin_src bash
# Example evaluation workflow
cd evaluation/scripts
./run_evaluation.sh --method ab-race --rubric code-generation
#+end_src

** 4. Analyze Results
- Use provided analysis templates
- Generate comparative reports
- Identify patterns and insights

* Best Practices

** Controlled Conditions
- Same hardware/environment
- Consistent time of day
- Identical starting conditions
- Minimal external factors

** Statistical Rigor
- Sufficient sample size (n≥5 minimum)
- Multiple trials per condition
- Account for variance
- Report confidence intervals

** Objective Scoring
- Clear scoring criteria
- Multiple evaluators when possible
- Calibration sessions
- Document edge cases

* Creating New Evaluations

** New Methods
1. Document theoretical basis
2. Define clear protocols
3. Pilot test methodology
4. Validate with team
5. Add to methods/

** New Rubrics
1. Identify evaluation needs
2. Define dimensions
3. Create scoring criteria
4. Calibrate with examples
5. Add to rubrics/

** New Scripts
1. Follow existing patterns
2. Make configurable
3. Add error handling
4. Document usage
5. Add to scripts/

* Integration with Experiments

Evaluations connect to the broader experimental framework:

#+begin_src org
  experiments/
  └── 2025-07-05-context-optimization/
      ├── setup.org           # References evaluation method
      ├── sessions/           # Raw data
      ├── results.org         # Uses evaluation rubrics
      └── evaluation/         # Applied evaluations
          ├── scores.org      # Rubric scores
          └── analysis.org    # Method-specific analysis
#+end_src

* Advanced Topics

** Automated Evaluation Pipeline
We're building towards:
- Continuous evaluation
- Automated scoring
- Trend detection
- Alert systems

** Meta-Evaluation
Evaluating our evaluations:
- Method effectiveness
- Rubric validity
- Inter-rater reliability
- Predictive power

** Cross-Experiment Analysis
Aggregate insights:
- Performance trends
- Model comparisons
- Method effectiveness
- Pattern identification

* Contributing

See [[../CONTRIBUTING.org][CONTRIBUTING.org]] for general guidelines.

For evaluations specifically:
1. Discuss new methods in issues first
2. Validate rubrics with team
3. Share pilot results
4. Document thoroughly
5. Consider automation

* Resources

- [[./methods/experimental-evaluation-methods.org][Experimental Methods Guide]]
- [[./rubrics/][Available Rubrics]]
- [[../docs/best-practices.org][Best Practices]]
- Statistical analysis guides
- Industry benchmarks