#+TITLE: Experimental Evaluation Methods

* Overview

These evaluation methods are designed to systematically compare LLM agents (particularly Opus vs Sonnet) and identify optimal prompting/guidance strategies for Claude Code. Each method provides different insights into agent capabilities and improvement strategies.

* Method 1: Race

** Concept
Run parallel sessions with identical goals but different parameters. Measure both efficiency and quality of solutions.

** Process
1. Start both sessions simultaneously
2. Provide identical initial context
3. Allow natural progression (minimal intervention)
4. Intervene only when:
   - Agent is clearly stuck
   - About to cause damage
   - Explicitly asks for clarification
5. Record all interventions with timestamps

** Scoring
Basic idea is to score based on number of interventions and clock time to achieve success.
Hence the 'race' title.

Claude also suggested a formula:
#+begin_src python
def calculate_race_score(turns, clock_time, interventions, quality_score):
    """
    Lower score is better (golf scoring)
    """
    base_score = turns * 10 + (clock_time / 60)  # minutes
    intervention_penalty = interventions * 20
    quality_multiplier = 2.0 - (quality_score / 100)  # 0-100% quality
    
    return (base_score + intervention_penalty) * quality_multiplier
#+end_src

*** Analysis Dimensions
- **Efficiency**: Turns and time to completion
- **Independence**: Number of interventions required
- **Error Patterns**: Types of mistakes made
- **Recovery**: How well agents self-correct
- **Strategy**: Different approaches taken


* Method 2: Iterative Refinement (Retries)

** Concept
Systematically improve task specifications based on failure analysis until achieving one-shot success.


** Process
1. **Attempt**: Run agent with current specification
2. **Record**: Document exact failures/mistakes
3. **Analyze**: Identify root causes
4. **Update**: Enhance specification to address issues
5. **Loop**: Until one-shot success achieved

*** Automated Analysis Prompt
#+begin_example
Given this task specification:
[Current specification]

And this failed attempt:
[Session log]

Analyze what went wrong and suggest specific additions/clarifications to the specification that would prevent these failures. Focus on:
1. Ambiguities that led to misinterpretation
2. Missing constraints or requirements
3. Unclear success criteria
4. Assumed context that wasn't provided
#+end_example


*** Metrics
- **Convergence Rate**: Iterations to success
- **Specification Growth**: Size/complexity increase
- **Failure Categories**: Common missing elements
- **Transferability**: Do improvements generalize?

* Method 3: Hint-Based Interventions

** Concept
Use a second agent or pattern matching to provide minimal hints when the primary agent encounters known issues.

The second agent is either smarter, has a working example to compare against, or collects problem/answer pairs from human operator.

** Process
1. Primary agent attempts task
2. Monitor for stuck states or errors
3. Intervention system analyzes current state
4. If confident or has a match: provide hint automatically
5. If uncertain: escalate to human for new pattern
6. Continue until task complete

*** Metrics
- **Intervention Count**: Average per task type
- **Pattern Coverage**: % of errors handled automatically
- **Hint Effectiveness**: Success rate after hints
- **Learning Curve**: Reduction in interventions over time


* Method 4: Comprehensive Partial Credit

** Concept
Evaluate attempts against detailed rubrics even if the task isn't fully completed, providing nuanced performance metrics.

The rubric could be evaluated by a second agent, and could potentially be derived from the spec directly if detailed enough. E.g. functional requirements or test cases.
